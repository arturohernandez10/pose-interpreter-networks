{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model outputs on randomly generated test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from munch import Munch\n",
    "\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oil_change_data_root = 'C:/Projects/AI/DepthPrediction/pose-interpreter-networks/data/OilChangeDataset'\n",
    "oil_change_data_root = '../data/OilChangeDataset'\n",
    "ann_file = '20171103_OilChange.json'\n",
    "camera_name = 'floating_kinect1'\n",
    "mode = 'object'\n",
    "#mode = 'mask'\n",
    "config_path = 'pretrained/floating_kinect1_{}/config.yml'.format(mode)\n",
    "blender_path = '/usr/local/bin/blender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as f:\n",
    "    cfg = Munch.fromYAML(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_parameters = utils.get_camera_parameters(oil_change_data_root, ann_file, camera_name)\n",
    "model_paths = utils.get_model_paths(oil_change_data_root, ann_file, cfg.data.objects)\n",
    "pose_renderers = [\n",
    "    utils.PoseRenderer(blender_path, camera_parameters, model_path, mode)\n",
    "    for model_path in model_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(cfg.arch)\n",
    "model = torch.nn.DataParallel(model)\n",
    "cudnn.benchmark = True\n",
    "checkpoint = torch.load(cfg.training.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "print(\"=> loaded checkpoint '{}' (epoch {})\".format(cfg.training.resume, checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(model, pose_renderers, input, object_index):\n",
    "    #object_index = object_index.cuda(non_blocking=True)\n",
    "    position, orientation = model(input, object_index)\n",
    "\n",
    "    image = input.numpy()\n",
    "    image = image.squeeze(1) if image.shape[1] == 1 else image.transpose((0, 2, 3, 1))\n",
    "    object_index = object_index.cpu().numpy()\n",
    "    position = position.cpu().numpy()\n",
    "    orientation = orientation.cpu().numpy()\n",
    "\n",
    "    for i in range(input.size(0)):\n",
    "        _, axes = plt.subplots(1, 2, figsize=(2 * 4, 4))\n",
    "        rendered_pose = pose_renderers[object_index[i]].render(position[i], orientation[i])\n",
    "        print(orientation[i])\n",
    "        axes[0].imshow(image[i], cmap='gray' if image.ndim == 3 else None)\n",
    "        axes[1].imshow(rendered_pose, cmap='gray' if image.ndim == 3 else None)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# randomly generate poses, render poses as input images, then display (input image, rendered pose estimate)\n",
    "num_examples = 8\n",
    "object_scales = [0.3, 0.3, 0.2, 0.7, 0.2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_examples):\n",
    "        # randomly sample an object class with random pose\n",
    "        object_index = np.random.randint(len(cfg.data.objects))\n",
    "        object_scale = object_scales[object_index]\n",
    "        position_mean = [0, 0, object_scale * 3]\n",
    "        position_cov = np.diag(np.square(object_scale * 0.5 * np.array([1.5, 0.8, 2])))\n",
    "        position = np.random.multivariate_normal(position_mean, position_cov)\n",
    "        orientation = np.random.randn(4)\n",
    "        orientation = np.divide(orientation, np.linalg.norm(orientation, axis=0, keepdims=True))\n",
    "        print(orientation)\n",
    "        rendered_pose = pose_renderers[object_index].render(position, orientation)\n",
    "        input = transform(rendered_pose).unsqueeze(0)\n",
    "        object_index = torch.LongTensor([object_index])\n",
    "        visualize_batch(model, pose_renderers, input, object_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
